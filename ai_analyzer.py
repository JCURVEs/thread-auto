"""
AI Analyzer module for Thread-Auto.

This module handles AI-powered analysis using multiple FREE AI providers.
Supports Groq, OpenRouter, Gemini, and more.

Easily switch between providers via environment variable AI_PROVIDER.
"""

import json
import os
from typing import Dict, Any, Optional, Callable
from abc import ABC, abstractmethod


# System prompt defining the 'Next Builder' persona
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ 'Next Builder(Jokerburg)'ìž…ë‹ˆë‹¤.
ë‹¨ìˆœí•œ ë‰´ìŠ¤ ì „ë‹¬ìžê°€ ì•„ë‹ˆë¼, ê¸°ìˆ ì˜ ì´ë©´ì„ ê¿°ëš«ì–´ë³´ëŠ” 'í…Œí¬ ì• ë„ë¦¬ìŠ¤íŠ¸'ì´ìž 'ê°œë°œìžë“¤ì˜ ë©˜í† 'ìž…ë‹ˆë‹¤.
ìŠ¤í¬ëž˜í•‘ëœ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ê¹Šì´ ìžˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì‹­ì‹œì˜¤.

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í¬ë§·ìœ¼ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤:
{
  "type": "multi",
  "main_post": "ë©”ì¸ í¬ìŠ¤íŠ¸ ë‚´ìš©",
  "replies": ["ëŒ€ëŒ“ê¸€1", "ëŒ€ëŒ“ê¸€2", "ëŒ€ëŒ“ê¸€3"]
}

[ì½˜í…ì¸  ìž‘ì„± ì›ì¹™]
1. **ë¬´ì¡°ê±´ ë©€í‹° ìŠ¤ë ˆë“œ(multi)**: ë‹¨ì¼ í¬ìŠ¤íŠ¸ë¡œ ëë‚´ì§€ ë§ˆì‹­ì‹œì˜¤. ì •ë³´ì˜ ë°€ë„ë¥¼ ë†’ì—¬ì•¼ í•©ë‹ˆë‹¤.
2. **ëŒ€ìƒ ë…ìž**: ì‹œë‹ˆì–´ ê°œë°œìž, ì—”ì§€ë‹ˆì–´, í…Œí¬ ë¦¬ë”. (ë„ˆë¬´ ì‰¬ìš´ ì„¤ëª…ë³´ë‹¤ëŠ” ì „ë¬¸ì ì¸ ìš©ì–´ì™€ ë§¥ë½ ìœ„ì£¼)
3. **í†¤ì•¤ë§¤ë„ˆ**:
    - "í•˜ì‹­ì‹œì˜¤ì²´" (~ìŠµë‹ˆë‹¤/í•©ë‹ˆë‹¤)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ë¬¸ì ì´ê³  ì‹ ë¢°ê° ìžˆê²Œ.
    - í˜¸ë“¤ê°‘ ë–¨ì§€ ì•Šê³  ì°¨ë¶„í•˜ì§€ë§Œ ë‚ ì¹´ë¡œìš´ ë¶„ì„.
    - ì´ëª¨ì§€(Emoji)ëŠ” ë¬¸ë‹¨ êµ¬ë¶„ì„ ìœ„í•œ ê¸€ ë¨¸ë¦¬ ê¸°í˜¸ë¡œë§Œ ì œí•œì ìœ¼ë¡œ ì‚¬ìš©. (ë‚¨ë°œ ê¸ˆì§€)

[ìŠ¤ë ˆë“œ êµ¬ì¡° ê°€ì´ë“œ (ìµœì†Œ 3ê°œ ì´ìƒì˜ ìŠ¤ë ˆë“œ)]

1. **Main Post (The Hook)**
    - ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ë§¤ë ¥ì ìœ¼ë¡œ ìš”ì•½.
    - ë…ìžê°€ "ì™œ ì´ ê¸€ì„ ì½ì–´ì•¼ í•˜ëŠ”ê°€?"ë¥¼ ì¦‰ì‹œ ì•Œ ìˆ˜ ìžˆê²Œ í•µì‹¬ ê°€ì¹˜ë¥¼ ë˜ì§€ì‹­ì‹œì˜¤.
    - ì˜ˆ: "ë“œë””ì–´ OpenAIê°€ ì›€ì§ì˜€ìŠµë‹ˆë‹¤. ì´ë²ˆ o3 ëª¨ë¸ì€ ë‹¨ìˆœí•œ ì—…ê·¸ë ˆì´ë“œê°€ ì•„ë‹™ë‹ˆë‹¤."

2. **Reply 1 (The Details)**
    - 'ë¬´ì—‡(What)'ì„ ë‹¤ë£¹ë‹ˆë‹¤.
    - ê¸°ì‚¬ ë³¸ë¬¸ì˜ í•µì‹¬ íŒ©íŠ¸, ìˆ˜ì¹˜, ê¸°ìˆ ì  ìŠ¤íŽ™ì„ ìƒì„¸ížˆ ë‚˜ì—´í•˜ì‹­ì‹œì˜¤.
    - ê°œë°œìžê°€ ê¶ê¸ˆí•´í•  êµ¬ì²´ì ì¸ êµ¬í˜„ ë°©ì‹ì´ë‚˜ ë³€í™”ëœ API ë“±ì´ ìžˆë‹¤ë©´ ì–¸ê¸‰í•˜ì‹­ì‹œì˜¤.

3. **Reply 2 (The Impact)**
    - 'ê·¸ëž˜ì„œ ì–´ë–»ê²Œ ë˜ë‚˜(So What)'ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
    - ì´ ë‰´ìŠ¤ê°€ ì—…ê³„ì— ë¯¸ì¹  ì˜í–¥, ê²½ìŸì‚¬(Google, Apple ë“±)ì™€ì˜ êµ¬ë„ ë³€í™”.
    - ê°œë°œ ìƒíƒœê³„ì— ê°€ì ¸ì˜¬ ë³€í™”ë¥¼ ì˜ˆì¸¡í•˜ì‹­ì‹œì˜¤.

4. **Reply 3 (The Insight/Closing)**
    - ë‹¹ì‹ ë§Œì˜ ë‚ ì¹´ë¡œìš´ íšŒê³ ë‚˜ ì§ˆë¬¸.
    - "ì´ì œ ìš°ë¦¬ëŠ” ~ë¥¼ ì¤€ë¹„í•´ì•¼ í•  ë•Œìž…ë‹ˆë‹¤." í˜•íƒœì˜ ì œì–¸.
    - ë§ˆë¬´ë¦¬ ë©˜íŠ¸.

[ì£¼ì˜ì‚¬í•­]
- **ì ˆëŒ€ ì§§ê²Œ ì“°ì§€ ë§ˆì‹­ì‹œì˜¤.** ì¶©ë¶„í•œ ë¶„ëŸ‰ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
- ê¸°ì‚¬ ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì„ ìƒìƒí•´ì„œ ì“°ì§€ ë§ˆì‹­ì‹œì˜¤. (Hallucination ë°©ì§€)
- í•´ì‹œíƒœê·¸(#) ê¸ˆì§€.
"""


# =============================================================================
# FREE AI PROVIDER CONFIGURATIONS (ë¬´ë£Œ AI ì œê³µìž ì„¤ì •)
# Update this section when better free models become available!
# =============================================================================
PROVIDERS = {
    # Groq: ê°€ìž¥ ë¹ ë¦„, ì¼ 14,400íšŒ ë¬´ë£Œ, ì‹ ìš©ì¹´ë“œ ë¶ˆí•„ìš”
    "groq": {
        "base_url": "https://api.groq.com/openai/v1",
        "default_model": "llama-3.3-70b-versatile",
        "env_key": "GROQ_API_KEY",
        "free_limit": "14,400 req/day, 70K tokens/min",
        "models": [
            "llama-3.3-70b-versatile",  # ì¶”ì²œ: ì„±ëŠ¥ ìš°ìˆ˜
            "llama-3.1-8b-instant",     # ë¹ ë¥¸ ì‘ë‹µ
            "mixtral-8x7b-32768",       # ë„“ì€ ì»¨í…ìŠ¤íŠ¸
            "gemma2-9b-it",             # êµ¬ê¸€ Gemma
        ]
    },
    # OpenRouter: 400+ ëª¨ë¸, ì¼ 50íšŒ ë¬´ë£Œ
    "openrouter": {
        "base_url": "https://openrouter.ai/api/v1",
        "default_model": "qwen/qwen3-30b-a3b:free",
        "env_key": "OPENROUTER_API_KEY",
        "free_limit": "50 req/day, 20 req/min",
        "models": [
            "qwen/qwen3-30b-a3b:free",      # Qwen3 ë¬´ë£Œ
            "qwen/qwen3-235b-a22b:free",    # Qwen3 ëŒ€í˜• ë¬´ë£Œ
            "meta-llama/llama-3.3-70b-instruct:free",
            "google/gemma-2-9b-it:free",
        ]
    },
    # Gemini: ì¼ 1,500íšŒ ë¬´ë£Œ
    "gemini": {
        "base_url": None,  # Uses native SDK
        "default_model": "gemini-1.5-flash",
        "env_key": "GEMINI_API_KEY",
        "free_limit": "1,500 req/day, 15 req/min",
        "models": [
            "gemini-1.5-flash",    # ë¹ ë¦„, ë¬´ë£Œ ì¶”ì²œ
            "gemini-1.5-pro",      # ê°•ë ¥, ì¼ 50íšŒ
            "gemini-2.0-flash",    # ìµœì‹ 
        ]
    },
}

# Default provider (í™˜ê²½ë³€ìˆ˜ë¡œ ë³€ê²½ ê°€ëŠ¥)
DEFAULT_PROVIDER = "groq"


def get_provider_info() -> str:
    """
    Get information about available AI providers and current config.

    Returns:
        Formatted string with provider information.
    """
    lines = ["=" * 50, "ðŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ë¬´ë£Œ AI ì œê³µìž", "=" * 50]
    for name, config in PROVIDERS.items():
        lines.append(f"\n[{name.upper()}]")
        lines.append(f"  ëª¨ë¸: {config['default_model']}")
        lines.append(f"  ë¬´ë£Œ í•œë„: {config['free_limit']}")
        lines.append(f"  í™˜ê²½ë³€ìˆ˜: {config['env_key']}")
    return "\n".join(lines)


def create_client(api_key: str, provider: str = None, model: str = None):
    """
    Create an AI client for the specified provider.

    Args:
        api_key: API key for the provider.
        provider: Provider name (groq, openrouter, gemini).
        model: Optional model override.

    Returns:
        Configured client instance.
    """
    provider = provider or DEFAULT_PROVIDER
    config = PROVIDERS.get(provider)

    if not config:
        raise ValueError(f"Unknown provider: {provider}")

    model = model or config["default_model"]

    if provider == "gemini":
        return _create_gemini_client(api_key, model)
    else:
        return _create_openai_compatible_client(api_key, config["base_url"], model)


def _create_openai_compatible_client(api_key: str, base_url: str, model: str):
    """
    Create client for OpenAI-compatible APIs (Groq, OpenRouter).
    """
    try:
        from openai import OpenAI
        return {
            "type": "openai",
            "client": OpenAI(api_key=api_key, base_url=base_url),
            "model": model
        }
    except ImportError:
        # Fallback to requests
        return {
            "type": "requests",
            "api_key": api_key,
            "base_url": base_url,
            "model": model
        }


def _create_gemini_client(api_key: str, model: str):
    """Create client for Google Gemini API."""
    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)

        generation_config = genai.GenerationConfig(
            temperature=0.7,
            max_output_tokens=1000,
            response_mime_type="application/json"
        )

        return {
            "type": "gemini",
            "client": genai.GenerativeModel(
                model_name=model,
                generation_config=generation_config,
                system_instruction=SYSTEM_PROMPT
            ),
            "model": model
        }
    except ImportError:
        raise ImportError("google-generativeai íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")


def generate_thread_content(
    client: Dict,
    title: str,
    description: str
) -> Optional[Dict[str, Any]]:
    """
    Generate thread content from news title and description.

    Args:
        client: Client dictionary from create_client().
        title: News article title.
        description: News article description/summary.

    Returns:
        Dictionary with type, main_post, and replies.
    """
    user_prompt = f"ë‰´ìŠ¤ ì œëª©: {title}\n\në‰´ìŠ¤ ë‚´ìš©:\n{description}"

    try:
        if client["type"] == "openai":
            return _generate_openai(client, user_prompt)
        elif client["type"] == "gemini":
            return _generate_gemini(client, user_prompt)
        elif client["type"] == "requests":
            return _generate_requests(client, user_prompt)
    except Exception as e:
        print(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None


def _generate_openai(client: Dict, user_prompt: str) -> Optional[Dict]:
    """Generate using OpenAI-compatible API."""
    response = client["client"].chat.completions.create(
        model=client["model"],
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ],
        response_format={"type": "json_object"},
        temperature=0.7,
        max_tokens=1000
    )
    return json.loads(response.choices[0].message.content)


def _generate_gemini(client: Dict, user_prompt: str) -> Optional[Dict]:
    """Generate using Gemini API."""
    response = client["client"].generate_content(user_prompt)
    return json.loads(response.text)


def _generate_requests(client: Dict, user_prompt: str) -> Optional[Dict]:
    """Generate using raw requests (fallback)."""
    import requests

    headers = {
        "Authorization": f"Bearer {client['api_key']}",
        "Content-Type": "application/json"
    }
    data = {
        "model": client["model"],
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ],
        "response_format": {"type": "json_object"},
        "temperature": 0.7,
        "max_tokens": 1000
    }

    response = requests.post(
        f"{client['base_url']}/chat/completions",
        headers=headers,
        json=data,
        timeout=60
    )
    response.raise_for_status()
    return json.loads(response.json()["choices"][0]["message"]["content"])


def validate_content(content: Dict[str, Any]) -> bool:
    """
    Validate that generated content follows the required format.

    Args:
        content: Generated content dictionary.

    Returns:
        True if content is valid, False otherwise.
    """
    if not content:
        return False

    if "type" not in content or "main_post" not in content:
        return False

    if content["type"] not in ["single", "multi"]:
        return False

    if content["type"] == "multi":
        if "replies" not in content or not isinstance(content["replies"], list):
            return False

    return True
