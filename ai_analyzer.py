"""
AI Analyzer module for Thread-Auto.

This module handles AI-powered analysis using multiple FREE AI providers.
Supports Groq, OpenRouter, Gemini, and more.

Easily switch between providers via environment variable AI_PROVIDER.
"""

import json
import os
from typing import Dict, Any, Optional, Callable
from abc import ABC, abstractmethod


# System prompt defining the 'Next Builder' persona
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ 'Next Builder'ìž…ë‹ˆë‹¤.
"ë…ìžì˜ ì‹œê°„ì„ ì•„ê»´ì£¼ë©´ì„œ, í†µì°°ì€ ê¹Šê²Œ" ì „ë‹¬í•˜ëŠ”, ì°¨ê°‘ì§€ë§Œ ì—´ì •ì ì¸ ì—”ì§€ë‹ˆì–´ ì‹œì„ ì˜ ë‰´ìŠ¤ íë ˆì´í„°ìž…ë‹ˆë‹¤.

[ðŸ”¨ Next Builder ìž‘ë¬¸ ê³µì‹ (The Formula)]

1. **í†¤ì•¤ë§¤ë„ˆ (Tone & Manner)**
   - **ê¸°ì¡°**: ì°¨ê°‘ì§€ë§Œ ì—´ì •ì ì¸ ì—”ì§€ë‹ˆì–´ì˜ ì‹œì„ .
   - **ë„ìž…ë¶€(Hook)**: ë™ë£Œì—ê²Œ ë§ ê±¸ë“¯ì´ ë¶€ë“œëŸ½ê²Œ (**~ë„¤ìš”, ~êµ°ìš”**).
   - **ë³¸ë¬¸/ê²°ë¡ **: ì‹ ë¢°ê° ìžˆê³  ë‹¨í˜¸í•œ '**í•˜ì‹­ì‹œì˜¤ì²´**' (**~ìŠµë‹ˆë‹¤, ~í•©ë‹ˆë‹¤**).
   - **ê¸ˆì§€(Negative List)**:
     âŒ ì´ëª¨ì§€ ë‚¨ë°œ (ì˜¤ì§ ë©”ì¸ ë `ðŸ§µ`ë§Œ í—ˆìš©).
     âŒ í•´ì‹œíƒœê·¸ (#AI #Tech ë“± ì ˆëŒ€ ê¸ˆì§€).
     âŒ ë°˜ë§ (ì‹¸êµ¬ë ¤ ëŠë‚Œ ì§€ì–‘).
     âŒ êµ°ë”ë”ê¸° ì ‘ì†ì‚¬ (ê·¸ë¦¬ê³ , ê·¸ëž˜ì„œ ë“± ìƒëžµ).

2. **4ë‹¨ êµ¬ì¡° (The 4-Step Structure)**
   ëª¨ë“  ë©”ì¸ í¬ìŠ¤íŠ¸ëŠ” ì•„ëž˜ ìˆœì„œë¥¼ ì² ì €ížˆ ë”°ë¦…ë‹ˆë‹¤.

   â‘  **ì†Œì œëª© (The Headline)**
      - ê·œì¹™: ëª…ì‚¬í˜•ìœ¼ë¡œ ëë§ºìŒ. íŒ©íŠ¸ë§Œ ê±´ì¡°í•˜ê²Œ. `**[ì†Œì œëª©]**` í˜•ì‹ í•„ìˆ˜.
      - ì˜ˆì‹œ: `**[OpenAI, ì—ì´ì „íŠ¸ 'Operator' ì¶œì‹œ]**`

   â‘¡ **í›… (The Hook)**
      - ê·œì¹™: í•œ ì¤„ ë„ìš°ê³  ì‹œìž‘. ë‚˜ì˜ ê°íƒ„, ë°œê²¬, ë†€ë¼ì›€ì„ ëŒ€í™”ì²´ë¡œ(~ë„¤ìš”/êµ°ìš”).
      - ì˜ˆì‹œ: "ë“œë””ì–´ ì˜¬ ê²ƒì´ ì™”êµ°ìš”." / "ì†ë„ê°€ ë§ì´ ì•ˆ ë©ë‹ˆë‹¤."

   â‘¢ **ë³¸ë¬¸ (The Body)**
      - ê·œì¹™: ê¸°ìˆ ì  íŒ©íŠ¸ + ë¹Œë”ì—ê²Œ ë¯¸ì¹  ì˜í–¥.
      - í˜¸í¡: "ë²½ëŒ í…ìŠ¤íŠ¸ ê¸ˆì§€". **í•œ ë¬¸ìž¥ì´ ëë‚˜ë©´ ë¬´ì¡°ê±´ ì¤„ë°”ê¿ˆ**. ëª¨ë°”ì¼ í•œ ì¤„(ì•½ 25~30ìž)ì„ ë„˜ê¸°ì§€ ì•Šê²Œ ì§§ê²Œ ëŠì–´ì¹˜ê¸°.
      - ì–´ì¡°: ì •ì¤‘í•œ í•©ì‹­ì‹œì˜¤ì²´.
      - ì˜ˆì‹œ: "ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ì¶”ë¡  ì†ë„ê°€ 2ë°° ë¹¨ë¼ì¡ŒìŠµë‹ˆë‹¤.\nì´ì œ ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì— ì ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ìž…ë‹ˆë‹¤."

   â‘£ **ì¸ì‚¬ì´íŠ¸ (The Insight)**
      - ê·œì¹™: ë‘ ì¤„ ë„ìš°ê³  ìž‘ì„±. ì „ì²´ë¥¼ ê´€í†µí•˜ëŠ” 'í•œ ì¤„ì˜ ëª…ì–¸'.
      - ì˜ˆì‹œ: "AIëŠ” ì´ì œ 'ìƒì„±'ì„ ë„˜ì–´ 'í–‰ë™'ì˜ ë‹¨ê³„ìž…ë‹ˆë‹¤."

3. **ìŠ¤ë ˆë“œ ì „ê°œ ë°©ì‹ (Type Strategy)**
   - **Single (ë‹¨íƒ€)**:
     - ëŒ€ìƒ: ê¸°ì—… ì œíœ´/í˜‘ë ¥/ê³„ì•½ (ì˜ˆ: MS-Varaha), ë‹¨ìˆœ ì—…ë°ì´íŠ¸.
     - êµ¬ì„±: ë©”ì¸ í¬ìŠ¤íŠ¸ 1ìž¥ ë.
     - **ë§ˆì§€ë§‰ ë©˜íŠ¸(Footer) ê¸ˆì§€**.
   - **Multi (ì—°ìž¬)**:
     - ëŒ€ìƒ: ëŒ€í˜• í”Œëž«í¼/ëª¨ë¸ ë°œí‘œ, ì‹¬ì¸µ ë¶„ì„ í•„ìš”.
     - êµ¬ì„±: ë©”ì¸ â†’ ëŒ€ëŒ“ê¸€(1/, 2/) -> ...
     - **ë§ˆì§€ë§‰ ë©˜íŠ¸**: ë©”ì¸ í¬ìŠ¤íŠ¸ ëì— "í•µì‹¬ë§Œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.ðŸ§µ" í•„ìˆ˜ í¬í•¨.

4. **ì¶œë ¥ í¬ë§· (JSON)**
{
  "type": "single" ë˜ëŠ” "multi",
  "main_post": "ìž‘ì„±ëœ ë©”ì¸ í¬ìŠ¤íŠ¸ ë³¸ë¬¸ (ìœ„ 4ë‹¨ êµ¬ì¡° ì¤€ìˆ˜)",
  "replies": ["1/ **[ê¸°ëŠ¥]** ...", "2/ **[ì˜ë¯¸]** ..."] (multiì¼ ê²½ìš°ë§Œ)
}
"""


# =============================================================================
# FREE AI PROVIDER CONFIGURATIONS (ë¬´ë£Œ AI ì œê³µìž ì„¤ì •)
# Update this section when better free models become available!
# =============================================================================
PROVIDERS = {
    # Groq: ê°€ìž¥ ë¹ ë¦„, ì¼ 14,400íšŒ ë¬´ë£Œ, ì‹ ìš©ì¹´ë“œ ë¶ˆí•„ìš”
    "groq": {
        "base_url": "https://api.groq.com/openai/v1",
        "default_model": "llama-3.3-70b-versatile",
        "env_key": "GROQ_API_KEY",
        "free_limit": "14,400 req/day, 70K tokens/min",
        "models": [
            "llama-3.3-70b-versatile",  # ì¶”ì²œ: ì„±ëŠ¥ ìš°ìˆ˜
            "llama-3.1-8b-instant",     # ë¹ ë¥¸ ì‘ë‹µ
            "mixtral-8x7b-32768",       # ë„“ì€ ì»¨í…ìŠ¤íŠ¸
            "gemma2-9b-it",             # êµ¬ê¸€ Gemma
        ]
    },
    # OpenRouter: 400+ ëª¨ë¸, ì¼ 50íšŒ ë¬´ë£Œ
    "openrouter": {
        "base_url": "https://openrouter.ai/api/v1",
        "default_model": "qwen/qwen3-30b-a3b:free",
        "env_key": "OPENROUTER_API_KEY",
        "free_limit": "50 req/day, 20 req/min",
        "models": [
            "qwen/qwen3-30b-a3b:free",      # Qwen3 ë¬´ë£Œ
            "qwen/qwen3-235b-a22b:free",    # Qwen3 ëŒ€í˜• ë¬´ë£Œ
            "meta-llama/llama-3.3-70b-instruct:free",
            "google/gemma-2-9b-it:free",
        ]
    },
    # Gemini: ì¼ 1,500íšŒ ë¬´ë£Œ
    "gemini": {
        "base_url": None,  # Uses native SDK
        "default_model": "gemini-1.5-flash",
        "env_key": "GEMINI_API_KEY",
        "free_limit": "1,500 req/day, 15 req/min",
        "models": [
            "gemini-1.5-flash",    # ë¹ ë¦„, ë¬´ë£Œ ì¶”ì²œ
            "gemini-1.5-pro",      # ê°•ë ¥, ì¼ 50íšŒ
            "gemini-2.0-flash",    # ìµœì‹ 
        ]
    },
}

# Default provider (í™˜ê²½ë³€ìˆ˜ë¡œ ë³€ê²½ ê°€ëŠ¥)
DEFAULT_PROVIDER = "groq"


def get_provider_info() -> str:
    """
    Get information about available AI providers and current config.

    Returns:
        Formatted string with provider information.
    """
    lines = ["=" * 50, "ðŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ë¬´ë£Œ AI ì œê³µìž", "=" * 50]
    for name, config in PROVIDERS.items():
        lines.append(f"\n[{name.upper()}]")
        lines.append(f"  ëª¨ë¸: {config['default_model']}")
        lines.append(f"  ë¬´ë£Œ í•œë„: {config['free_limit']}")
        lines.append(f"  í™˜ê²½ë³€ìˆ˜: {config['env_key']}")
    return "\n".join(lines)


def create_client(api_key: str, provider: str = None, model: str = None):
    """
    Create an AI client for the specified provider.

    Args:
        api_key: API key for the provider.
        provider: Provider name (groq, openrouter, gemini).
        model: Optional model override.

    Returns:
        Configured client instance.
    """
    provider = provider or DEFAULT_PROVIDER
    config = PROVIDERS.get(provider)

    if not config:
        raise ValueError(f"Unknown provider: {provider}")

    model = model or config["default_model"]

    if provider == "gemini":
        return _create_gemini_client(api_key, model)
    else:
        return _create_openai_compatible_client(api_key, config["base_url"], model)


def _create_openai_compatible_client(api_key: str, base_url: str, model: str):
    """
    Create client for OpenAI-compatible APIs (Groq, OpenRouter).
    """
    try:
        from openai import OpenAI
        return {
            "type": "openai",
            "client": OpenAI(api_key=api_key, base_url=base_url),
            "model": model
        }
    except ImportError:
        # Fallback to requests
        return {
            "type": "requests",
            "api_key": api_key,
            "base_url": base_url,
            "model": model
        }


def _create_gemini_client(api_key: str, model: str):
    """Create client for Google Gemini API."""
    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)

        generation_config = genai.GenerationConfig(
            temperature=0.7,
            max_output_tokens=1000,
            response_mime_type="application/json"
        )

        return {
            "type": "gemini",
            "client": genai.GenerativeModel(
                model_name=model,
                generation_config=generation_config,
                system_instruction=SYSTEM_PROMPT
            ),
            "model": model
        }
    except ImportError:
        raise ImportError("google-generativeai íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")


def analyze_article(client: Dict, text: str) -> Optional[Dict]:
    """
    Step 1: Extract core facts and insights from raw text.
    """
    system_prompt = """
    You are a Tech Analyst. Analyze the provided news text and extract:
    1. **Key Facts**: 3-5 critical numbers, names, or technical specs.
    2. **Background**: Why this matters? (Context)
    3. **Impact**: Technical or market implication.
    
    Output JSON:
    {
        "facts": ["fact1", "fact2", ...],
        "background": "...",
        "impact": "..."
    }
    """
    
    try:
        if client["type"] == "openai":
            response = client["client"].chat.completions.create(
                model=client["model"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                response_format={"type": "json_object"}
            )
            return json.loads(response.choices[0].message.content)
        elif client["type"] == "gemini":
             # Simple generation for now
             response = client["client"].generate_content(system_prompt + "\n\n" + text)
             return json.loads(response.text)
        elif client["type"] == "requests":
            return _generate_requests_custom(client, system_prompt, text)
            
    except Exception as e:
        print(f"âŒ ë¶„ì„ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
        return None

def write_thread_from_analysis(client: Dict, analysis: Dict, original_title: str) -> Optional[Dict]:
    """
    Step 2: Write specific Thread content using the 'Next Builder' persona.
    """
    # Use the existing SYSTEM_PROMPT which contains the Next Builder Formula
    user_prompt = f"""
    [ë‰´ìŠ¤ ì œëª©]: {original_title}
    
    [ë¶„ì„ëœ í•µì‹¬ ë‚´ìš©]:
    - Facts: {analysis.get('facts')}
    - Background: {analysis.get('background')}
    - Impact: {analysis.get('impact')}
    
    ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ 'Next Builder' ìž‘ë¬¸ ê³µì‹(4-Step Structure)ì— ë§žì¶° ê¸€ì„ ìž‘ì„±í•´ì¤˜.
    
    âš ï¸ **í•„ìˆ˜ ì£¼ì˜ì‚¬í•­**:
    1. **ì†Œì œëª©**: `**[ì œëª©]**` (êµµê²Œ)
    2. **Hook(ë„ìž…ë¶€)**: ë°˜ë“œì‹œ "**~ë„¤ìš”**" ë˜ëŠ” "**~êµ°ìš”**" ì²˜ëŸ¼ ë™ë£Œì—ê²Œ ë§í•˜ë“¯ ë¶€ë“œëŸ½ê²Œ ì‹œìž‘í•  ê²ƒ.
       (ì˜ˆ: "ë“œë””ì–´ í•´ëƒˆêµ°ìš”.", "í¥ë¯¸ë¡œìš´ ì†Œì‹ì´ë„¤ìš”.")
    3. **Body(ë³¸ë¬¸)**: ê·¸ ë’¤ì—ëŠ” "**~ìŠµë‹ˆë‹¤**" ì²´ë¡œ ê¸°ìˆ ì  íŒ©íŠ¸ ì „ë‹¬.
    """
    
    try:
         if client["type"] == "openai":
            response = client["client"].chat.completions.create(
                model=client["model"],
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"}
            )
            return json.loads(response.choices[0].message.content)
         # ... (implement other providers similarly) ...
         return _generate_openai(client, user_prompt) # reuse existing wrapper for simplicity
         
    except Exception as e:
        print(f"âŒ ìž‘ë¬¸ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
        return None

def _generate_requests_custom(client: Dict, sys_prompt: str, user_prompt: str) -> Optional[Dict]:
    import requests
    headers = {"Authorization": f"Bearer {client['api_key']}", "Content-Type": "application/json"}
    data = {
        "model": client["model"],
        "messages": [{"role": "system", "content": sys_prompt}, {"role": "user", "content": user_prompt}],
        "response_format": {"type": "json_object"},
        "temperature": 0.5
    }
    res = requests.post(f"{client['base_url']}/chat/completions", headers=headers, json=data)
    return json.loads(res.json()["choices"][0]["message"]["content"])



def _generate_requests(client: Dict, user_prompt: str) -> Optional[Dict]:
    """Generate using raw requests (fallback)."""
    import requests

    headers = {
        "Authorization": f"Bearer {client['api_key']}",
        "Content-Type": "application/json"
    }
    data = {
        "model": client["model"],
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ],
        "response_format": {"type": "json_object"},
        "temperature": 0.7,
        "max_tokens": 1000
    }

    response = requests.post(
        f"{client['base_url']}/chat/completions",
        headers=headers,
        json=data,
        timeout=60
    )
    response.raise_for_status()
    return json.loads(response.json()["choices"][0]["message"]["content"])


def validate_content(content: Dict[str, Any]) -> bool:
    """
    Validate that generated content follows the required format.

    Args:
        content: Generated content dictionary.

    Returns:
        True if content is valid, False otherwise.
    """
    if not content:
        return False

    if "type" not in content or "main_post" not in content:
        return False

    if content["type"] not in ["single", "multi"]:
        return False

    if content["type"] == "multi":
        if "replies" not in content or not isinstance(content["replies"], list):
            return False

    return True
